# Clutch.co Profile Scraper

[![Python Version](https://img.shields.io/badge/python-3.7%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A robust Python-based web scraping tool designed to extract company profile information and website URLs from Clutch.co profiles. This tool is particularly useful for lead generation, market research, and business intelligence gathering.

## âœ¨ Features

- ğŸš€ **Efficient Scraping**: Extract company website URLs from Clutch.co profiles
- ğŸ”„ **Resume Capability**: Continue interrupted scraping sessions
- ğŸ›¡ **Anti-detection**: Uses undetected-chromedriver to avoid bot detection
- ğŸ“Š **CSV Integration**: Easy import/export of data in CSV format
- âš¡ **Fast & Reliable**: Multi-threaded processing for better performance
- ğŸ“ **Detailed Logging**: Comprehensive status updates and error reporting

## ğŸ“‹ Prerequisites

- Python 3.7 or higher
- Google Chrome browser installed
- ChromeDriver (will be handled automatically by webdriver-manager)

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/clutch-profile-scraper.git
   cd clutch-profile-scraper
   ```

2. **Set up a virtual environment** (recommended)
   ```bash
   # For Windows
   python -m venv venv
   .\venv\Scripts\activate
   
   # For macOS/Linux
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸƒâ€â™‚ï¸ Quick Start

1. **Prepare your input file**
   - Place your CSV file with Clutch.co profile URLs in the `data` directory
   - Ensure the file has a column containing the profile URLs
   - The script will automatically detect URL columns

2. **Run the scraper**
   ```bash
   python app.py
   ```

3. **View results**
   - The script will create a `clutch_with_sites.csv` file with the results
   - Progress is saved automatically, so you can stop and resume anytime

## ğŸ›  Scripts

- `app.py`: Main scraping script
- `continue_scraping.py`: Resume interrupted scraping sessions
- `merge_csv.py`: Merge multiple CSV files
- `single_url.py`: Test scraping for a single URL
- `update_missing_urls.py`: Update missing URLs in the dataset

## âš™ï¸ Configuration

Edit the configuration section in `app.py` to customize:

```python
# Input/Output files
INPUT_CSV = "data/extracted_urls_20250604_161856.csv"  # Your input file
OUTPUT_CSV = "clutch_with_sites.csv"  # Output file
TEMP_CSV = "clutch_temp.csv"  # Temporary progress file

# Browser settings
HEADLESS = True  # Set to False to see the browser
WAIT_SHORT = 20  # Seconds to wait for elements
MAX_RETRIES = 2  # Number of retries for failed attempts
SLEEP_BETWEEN = (3, 6)  # Random delay between requests (min, max) seconds
```

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/                      # Directory for input/output files
â”‚   â””â”€â”€ extracted_urls_20250604_161856.csv  # Example input file
â”œâ”€â”€ app.py                    # Main scraping script
â”œâ”€â”€ app_clean.py              # Cleaned version of the main script
â”œâ”€â”€ continue_scraping.py      # Resume interrupted sessions
â”œâ”€â”€ merge_csv.py              # Merge multiple CSV files
â”œâ”€â”€ single_url.py             # Test script for single URL
â”œâ”€â”€ update_missing_urls.py    # Update missing URLs
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                # This file
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add some amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ’¡ Support

For support, please:
- Open an issue in the GitHub repository
- Or contact the maintainer directly

## ğŸ“Š Performance

- Processes approximately 100-200 profiles per hour (varies based on website response times)
- Includes automatic retries and error handling
- Saves progress to prevent data loss

## âš ï¸ Legal Notice

This tool is for educational and research purposes only. Please ensure you comply with Clutch.co's Terms of Service and use this tool responsibly. The developers are not responsible for any misuse or damage caused by this software.
